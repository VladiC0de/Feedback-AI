{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration der Sentiment-Analyse\n",
    "\n",
    "In diesem Notebook untersuchen wir verschiedene Modelle zur Sentiment-Analyse und evaluieren ihre Leistung auf unseren Kundenfeedback-Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Plotting-Einstellungen\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Daten laden\n",
    "\n",
    "Wir laden die Google-Reviews-Daten und bereiten sie für die Analyse vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten laden\n",
    "try:\n",
    "    df = pd.read_csv('../data/google_reviews.csv', encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv('../data/google_reviews.csv', encoding='latin-1')\n",
    "\n",
    "# Daten anzeigen\n",
    "print(f\"Anzahl der Reviews: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Datenexploration\n",
    "\n",
    "Wir untersuchen die Verteilung der Bewertungen und die Textlänge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textlänge berechnen\n",
    "df['text_length'] = df['text'].astype(str).apply(len)\n",
    "\n",
    "# Verteilung der Textlänge\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['text_length'], bins=30)\n",
    "plt.title('Verteilung der Textlänge')\n",
    "plt.xlabel('Anzahl der Zeichen')\n",
    "plt.ylabel('Anzahl der Reviews')\n",
    "plt.show()\n",
    "\n",
    "# Statistiken zur Textlänge\n",
    "df['text_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment-Modell laden\n",
    "\n",
    "Wir laden das vortrainierte BERT-Modell für die Sentiment-Analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell laden\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1  # CPU verwenden\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chunking-Funktion für lange Texte\n",
    "\n",
    "Da BERT eine maximale Eingabelänge hat, implementieren wir eine Chunking-Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_chunked(text):\n",
    "    \"\"\"Sentiment-Analyse mit Chunking für lange Texte\"\"\"\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "    ids = enc[\"input_ids\"][0]\n",
    "    chunks = [ids[i : i + 512] for i in range(0, len(ids), 512)]\n",
    "    results = []\n",
    "    \n",
    "    for c in chunks:\n",
    "        txt = tokenizer.decode(c, skip_special_tokens=True)\n",
    "        r = sentiment_pipe(txt, truncation=True, max_length=512, padding=True)[0]\n",
    "        results.append(r)\n",
    "    \n",
    "    labels = [r[\"label\"] for r in results]\n",
    "    maj = max(set(labels), key=labels.count)\n",
    "    avg = sum(r[\"score\"] for r in results) / len(results)\n",
    "    \n",
    "    return {\"label\": maj, \"score\": avg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sentiment-Analyse durchführen\n",
    "\n",
    "Wir wenden die Sentiment-Analyse auf unsere Daten an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment-Analyse für eine Stichprobe\n",
    "sample_size = min(50, len(df))\n",
    "sample_df = df.sample(sample_size, random_state=42)\n",
    "\n",
    "# Sentiment-Analyse durchführen\n",
    "results = []\n",
    "for text in sample_df['text'].astype(str):\n",
    "    result = sentiment_chunked(text)\n",
    "    results.append(result)\n",
    "\n",
    "# Ergebnisse zum DataFrame hinzufügen\n",
    "sample_df['sentiment_label'] = [r['label'] for r in results]\n",
    "sample_df['sentiment_score'] = [r['score'] for r in results]\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "sample_df[['text', 'sentiment_label', 'sentiment_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluierung der Sentiment-Analyse\n",
    "\n",
    "Wir evaluieren die Leistung des Modells anhand von manuell bewerteten Beispielen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Testdaten mit manuellen Labels\n",
    "test_data = [\n",
    "    {\"text\": \"Das Produkt ist fantastisch! Ich bin sehr zufrieden.\", \"expected\": \"5 stars\"},\n",
    "    {\"text\": \"Gutes Produkt, aber etwas teuer.\", \"expected\": \"4 stars\"},\n",
    "    {\"text\": \"Durchschnittliche Qualität, erfüllt seinen Zweck.\", \"expected\": \"3 stars\"},\n",
    "    {\"text\": \"Nicht besonders beeindruckt, hatte mehr erwartet.\", \"expected\": \"2 stars\"},\n",
    "    {\"text\": \"Schreckliches Produkt, funktioniert überhaupt nicht!\", \"expected\": \"1 stars\"},\n",
    "    {\"text\": \"Die Lieferung war schnell, aber das Produkt hat Mängel.\", \"expected\": \"3 stars\"},\n",
    "    {\"text\": \"Sehr gute Qualität und schneller Service.\", \"expected\": \"5 stars\"},\n",
    "    {\"text\": \"Enttäuschend, würde nicht wieder kaufen.\", \"expected\": \"2 stars\"},\n",
    "    {\"text\": \"Mittelmäßig, nichts Besonderes.\", \"expected\": \"3 stars\"},\n",
    "    {\"text\": \"Absolut begeistert von diesem Produkt!\", \"expected\": \"5 stars\"}\n",
    "]\n",
    "\n",
    "# Sentiment-Analyse für Testdaten\n",
    "for item in test_data:\n",
    "    result = sentiment_chunked(item[\"text\"])\n",
    "    item[\"predicted\"] = result[\"label\"]\n",
    "    item[\"score\"] = result[\"score\"]\n",
    "\n",
    "# Ergebnisse als DataFrame\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metriken berechnen\n",
    "y_true = test_df['expected']\n",
    "y_pred = test_df['predicted']\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=sorted(set(y_true)))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=sorted(set(y_true)), \n",
    "            yticklabels=sorted(set(y_true)))\n",
    "plt.xlabel('Vorhergesagt')\n",
    "plt.ylabel('Tatsächlich')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fazit\n",
    "\n",
    "Das BERT-Modell `nlptown/bert-base-multilingual-uncased-sentiment` zeigt gute Ergebnisse bei der Sentiment-Analyse von Kundenfeedback. Die Accuracy liegt bei etwa 80%, was für unseren Anwendungsfall ausreichend ist. Die Implementierung mit Chunking ermöglicht die Verarbeitung längerer Texte, was für Kundenfeedback wichtig ist.\n",
    "\n",
    "Verbesserungspotenzial:\n",
    "- Fine-Tuning des Modells mit domänenspezifischen Daten\n",
    "- Optimierung der Chunking-Strategie\n",
    "- Berücksichtigung von Kontext und Satzstruktur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
